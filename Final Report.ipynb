{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marvel Dialogue Classification\n",
    "#### CS 345 Final Project\n",
    "By: Preston Dunton  \n",
    "December 8, 2020\n",
    "\n",
    "<img src=\"https://blog.umhb.edu/wp-content/uploads/2019/06/mcu-1920x1080.jpg\" alt=\"MCU Banner\" width=\"60%\" height=\"60%\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook summarizes the project that can be found in [this repository](https://github.com/prestondunton/marvel-dialogue-nlp). This project is my final project for CS 345, Machine Learning Foundations and Practice, at Colorado State University (Fall 2020).  The goals of this project were for me to apply what I've learned this semester, as well as introduce me into NLP problems and methods.\n",
    "\n",
    "\n",
    "## The Problem\n",
    "The problem whished to be accomplished in this project is an NLP classification problem.  The goal was to create a model that can predict a character's name given a line of their dialogue from a Marvel Cinematic Universe (MCU) movie.  Data was taken from Marvel released scripts and transformed into labels of names and feature documents of their dialogue.\n",
    "\n",
    "\n",
    "## About the Dataset\n",
    "This repository contains a newly created dataset to train and test models on, as well as several Jupyter Notebooks that describe the process used to create each `.csv`.  These Jupyter notebooks explain the process of parsing the `.pdf`s with the `pandas` library.  The end file, [mcu.csv](https://github.com/prestondunton/marvel-dialogue-nlp/blob/master/data/mcu.csv), contains columns `character` and `line` that hold the dialogue for several movies from the MCU. There are more columns that provide additional features for context, but were not used in this project.  See [/data/MCU.ipynb](https://github.com/prestondunton/marvel-dialogue-nlp/blob/master/data/MCU.ipynb) for more details on those features. For individual movies, the corresponding `.csv` can be found in [/data/cleaned/](https://github.com/prestondunton/marvel-dialogue-nlp/blob/master/data/cleaned) and contain columns `character` and `line`.  Each movie file was created using the same partially automated process, though improvements were found as more movies were processed.\n",
    "\n",
    "The movie script `.pdf`s were obtained from [Script Slug](https://www.scriptslug.com/scripts/category/marvel), though other copies of the Marvel released scripts can be found online elsewhere.  Only a few of the MCU movie scripts were released, so this dataset only contains a subset of the movies in the MCU (listed below).  Transcripts exist for all 21 movies, though these transcripts can contain many errors, so they were not used.  Additionally, creating each `.csv` took quite a bit of time (approximately 12 hours per movie), so currently, this dataset only contains 5 movies (listed below).\n",
    "\n",
    "\n",
    "| MCU Movies on Script Slug             | Included in Project |\n",
    "| ------------------------------------- | ------------------- |\n",
    "| Iron Man (2008)                       | ✔️                 | \n",
    "| The Avengers (2012)                   | ✔️                 |\n",
    "| Thor: Ragnorak (2017)                 | ✔️                 |\n",
    "| Guardians of the Galaxy Vol. 2 (2017) | ✔️                 |\n",
    "| Avengers Endgame (2019)               | ✔️                 |\n",
    "| Thor (2011)                           | ❌                  |\n",
    "| Captain America (2011)                | ❌                  |\n",
    "| Black Panther (2018)                  | ❌                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "In order to accomplish the task described above, 12 models were created.  These models employ different combinations of NLP techniques and different ML classifiers.  A summary of the architecture of the 12 models, as well as an explaination of this project's use of each technique and classifier, is below.\n",
    "\n",
    "| Model # | Classifier    | Uses Stemming | Uses TF / IDF Transformation |\n",
    "| ------- | ------------- | -------- | ---------------------------- |\n",
    "| 1       | Naive Bayes   | ❌      | ❌                           |\n",
    "| 2       | Naive Bayes   | ❌      | ✔️                           |\n",
    "| 3       | Naive Bayes   | ✔️      | ❌                           |\n",
    "| 4       | Naive Bayes   | ✔️      | ✔️                           |\n",
    "| 5       | Random Forest | ❌      | ❌                           |\n",
    "| 6       | Random Forest | ❌      | ✔️                           |\n",
    "| 7       | Random Forest | ✔️      | ❌                           |\n",
    "| 8       | Random Forest | ✔️      | ✔️                           |\n",
    "| 9       | SVM           | ❌      | ❌                           |\n",
    "| 10      | SVM           | ❌      | ✔️                           |\n",
    "| 11      | SVM           | ✔️      | ❌                           |\n",
    "| 12      | SVM           | ✔️      | ✔️                           |\n",
    "\n",
    "\n",
    "## Grid Searching\n",
    "\n",
    "Scikit-learn provides an interface for model selection called `GridSearchCV` which uses cross validation to select the best combination of hyperparameters for a given model.  The first 11 of the models all use `GridSearchCV` to select the best performing parameters for the `CountVectorizer` (or the custom `StemCountVectorizer` if the model uses stemming).  Model 12 does not use GridSearchCV, because the estimated compute time for searching every combination of defined parameters was 90 hours. Instead, model 12 uses parameters from model 10, which is the same architecture except for the use of stemming.  It should also be noted that the first 11 models were trained from the same options of parameters, so that they could be compared.\n",
    "\n",
    "## Stop Words\n",
    "\n",
    "The removal of non-important words from the feature documents, called \"stop words,\" is often an important step in NLP problems.  Stop words like `[\"the\",\"I\", \"we\", \"she\" ...]` often don't provide any value to a model. Three options were provided to `GridSearchCV` for removing stop words.  The first is `None`, which means we don't remove stop words.  The other two options are two different sets of stop words, the `english` set provided by Scikit-learn, and the `english` set provided by NLTK, which was included because the documentation for Scikit-learn's [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) recommends a different set of stop words.\n",
    "\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Stemming is the process of taking derrived words, such as verb conjugations and plurals, and transforming them back into their base versions.  For example, \"fishing,\" \"fished,\" and \"fishes\" might all be stemmed into \"fish.\"  Stemming is useful in NLP because it allows our word count features to group derrived words by their base words.  In this project, NLTK's Snowball Stemmer is used in a custom class that extends Scikit-learn's `CountVectorizer`.  See Javed Shaikh's article in the Sources section for the inspiration for this approach.\n",
    "\n",
    "\n",
    "## TF / IDF Transformations\n",
    "\n",
    "\n",
    "## Naive Bayes Classifiers\n",
    "\n",
    "Naive Bayes Classification is based on Bayes's theorem, which says that \n",
    "\n",
    "$$\n",
    "p(y|\\textbf{x})=\\frac{p(\\textbf{x}|y)*p(y)}{p(\\textbf{x})}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$p(y|\\textbf{x})$ is our **posterior**, or probability of our label given an observation's features.\n",
    "\n",
    "$p(\\textbf{x}|y)$ is our **likelihood**, or the probability of our features given a observation's label.\n",
    "\n",
    "$p(y)$ is our **prior**, or the probability of our label without seeing any of the observation's features.  \n",
    "\n",
    "The use of Bayes's theorem in ML assumes that all of our features are independent, which they often aren't.  Especially in NLP, certain words are more likely when observing related words.  By ignoring this, we make Bayes's theorem \"naive\", and can compute  our model.\n",
    "\n",
    "In this project, we compute the probability of a line belonging to a certain character by treating each word in the text as a feature.  We then select the character with the highest posterior given the features as our prediction.\n",
    "\n",
    "\n",
    "## Support Vector Machine Classifiers\n",
    "\n",
    "\n",
    "## Random Forest Classifiers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "class StemCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemCountVectorizer, self).build_analyzer()\n",
    "        \n",
    "        return lambda document: ([SnowballStemmer('english', ignore_stopwords=True).stem(word) for word in analyzer(document)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Got it.', 'And terrifying.', 'What’s the delta rate?', ...,\n",
       "        'I’m sorry. He seemed like a good man.', 'Heimdall, come on.',\n",
       "        'I am a king!'], dtype='<U606'),\n",
       " array(['BRUCE BANNER', 'PEPPER POTTS', 'TONY STARK', ..., 'STEVE ROGERS',\n",
       "        'THOR', 'LOKI'], dtype='<U12'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcu = pd.read_csv(\"./data/mcu.csv\")\n",
    "\n",
    "mcu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Data Analysis\n",
    "\n",
    "Word Count distribution, Line count distribution, the number of charachters chosen and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Count Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_count = pd.DataFrame(mcu.groupby([\"movie\",\"character\"]).line.nunique())\n",
    "line_count.reset_index(inplace=True)\n",
    "line_count = line_count.pivot(index=\"character\", columns=\"movie\", values=\"line\")\n",
    "line_count.fillna(0, inplace=True)\n",
    "line_count[\"total\"] = line_count.sum(axis=1)\n",
    "line_count = line_count.astype(\"int64\")\n",
    "line_count.sort_values(by=\"total\", ascending=False)\n",
    "\n",
    "line_count['total'].hist(bins=60)\n",
    "pd.DataFrame(line_count['total']).describe(percentiles = DESCRIBE_PERCENTILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Per Line Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcu.hist(column=\"words\", bins=60)\n",
    "pd.DataFrame(mcu[\"words\"]).describe(percentiles = DESCRIBE_PERCENTILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_line_count = 150\n",
    "\n",
    "is_main_character = mcu[\"character\"].value_counts() > min_line_count\n",
    "is_main_character = is_main_character.rename(\"is main character\", axis=0)\n",
    "\n",
    "main_character_rows = is_main_character[mcu[\"character\"]]\n",
    "main_character_rows = main_character_rows.reset_index(drop=True)\n",
    "\n",
    "mcu = mcu[main_character_rows]\n",
    "\n",
    "y = mcu[\"character\"].to_numpy().astype(str)\n",
    "X = mcu[\"line\"].to_numpy().astype(str)\n",
    "\n",
    "X, y = shuffle(X, y, random_state=RANDOM_SEED)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validator = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED, shuffle=True)\n",
    "score_method = \"balanced_accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "stem_count_vectorizer = StemCountVectorizer()\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "rf_classifier = RandomForestClassifier(n_jobs=-1, random_state=RANDOM_SEED)\n",
    "svm_classifier = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_params = {'vect__binary': [True, False],\n",
    "               'vect__stop_words': [None, 'english', stopwords.words('english')],\n",
    "              'vect__ngram_range': [(1,1), (1,2), (1,3)]}\n",
    "\n",
    "tfidf_params = {'tfidf__norm': ['l1', 'l2'],\n",
    "              'tfidf__use_idf': [True, False]}\n",
    "\n",
    "nb_params = {'clf__alpha': [1, 1e-1, 1e-2, 1e-3],\n",
    "             'clf__fit_prior': [True, False]}\n",
    "\n",
    "rf_params = {'clf__criterion': [\"gini\", \"entropy\"],\n",
    "             'clf__max_depth': [None, 7, 8, 9, 10 ,11 ,12],\n",
    "             'clf__max_features': [None, \"sqrt\", \"log2\"],\n",
    "             'clf__class_weight': [None, 'balanced']}\n",
    "\n",
    "svm_params = {'clf__C': [1e-2, 1e-1, 0, 1, 10, 100],\n",
    "              'clf__kernel': ['linear', 'poly', 'rbf'],\n",
    "              'clf__degree': [2,3,4,5,6],\n",
    "              'clf__gamma': ['scale', 'auto'],\n",
    "              'clf__class_weight': [None, 'balanced']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 (Naive Bayes, no TFIDF, no stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:   11.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.1,\n",
       " 'clf__fit_prior': False,\n",
       " 'vect__binary': True,\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__stop_words': ['i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  \"you're\",\n",
       "  \"you've\",\n",
       "  \"you'll\",\n",
       "  \"you'd\",\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  \"she's\",\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  \"it's\",\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  \"that'll\",\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  \"don't\",\n",
       "  'should',\n",
       "  \"should've\",\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  \"aren't\",\n",
       "  'couldn',\n",
       "  \"couldn't\",\n",
       "  'didn',\n",
       "  \"didn't\",\n",
       "  'doesn',\n",
       "  \"doesn't\",\n",
       "  'hadn',\n",
       "  \"hadn't\",\n",
       "  'hasn',\n",
       "  \"hasn't\",\n",
       "  'haven',\n",
       "  \"haven't\",\n",
       "  'isn',\n",
       "  \"isn't\",\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  \"mightn't\",\n",
       "  'mustn',\n",
       "  \"mustn't\",\n",
       "  'needn',\n",
       "  \"needn't\",\n",
       "  'shan',\n",
       "  \"shan't\",\n",
       "  'shouldn',\n",
       "  \"shouldn't\",\n",
       "  'wasn',\n",
       "  \"wasn't\",\n",
       "  'weren',\n",
       "  \"weren't\",\n",
       "  'won',\n",
       "  \"won't\",\n",
       "  'wouldn',\n",
       "  \"wouldn't\"]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1 = Pipeline([('vect', count_vectorizer), \n",
    "                  ('clf', nb_classifier)])\n",
    "\n",
    "parameters1 = {**count_params, **nb_params}\n",
    "\n",
    "grid1 = GridSearchCV(pipe1, parameters1, cv=cross_validator, scoring=score_method, n_jobs=-1, verbose=3)\n",
    "\n",
    "grid1.fit(X,y)\n",
    "\n",
    "grid1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Pipeline([('vect', CountVectorizer(binary=True, ngram_range = (1,3), stop_words = stopwords.words('english'))),\n",
    "                  ('clf', MultinomialNB(alpha=0.1, fit_prior=False))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 (Naive Bayes, TFIDF, no stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 952 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1528 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2232 tasks      | elapsed:   32.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed:   42.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.1,\n",
       " 'clf__fit_prior': False,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__binary': False,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__stop_words': ['i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  \"you're\",\n",
       "  \"you've\",\n",
       "  \"you'll\",\n",
       "  \"you'd\",\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  \"she's\",\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  \"it's\",\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  \"that'll\",\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  \"don't\",\n",
       "  'should',\n",
       "  \"should've\",\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  \"aren't\",\n",
       "  'couldn',\n",
       "  \"couldn't\",\n",
       "  'didn',\n",
       "  \"didn't\",\n",
       "  'doesn',\n",
       "  \"doesn't\",\n",
       "  'hadn',\n",
       "  \"hadn't\",\n",
       "  'hasn',\n",
       "  \"hasn't\",\n",
       "  'haven',\n",
       "  \"haven't\",\n",
       "  'isn',\n",
       "  \"isn't\",\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  \"mightn't\",\n",
       "  'mustn',\n",
       "  \"mustn't\",\n",
       "  'needn',\n",
       "  \"needn't\",\n",
       "  'shan',\n",
       "  \"shan't\",\n",
       "  'shouldn',\n",
       "  \"shouldn't\",\n",
       "  'wasn',\n",
       "  \"wasn't\",\n",
       "  'weren',\n",
       "  \"weren't\",\n",
       "  'won',\n",
       "  \"won't\",\n",
       "  'wouldn',\n",
       "  \"wouldn't\"]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2 = Pipeline([('vect', count_vectorizer),\n",
    "                  ('tfidf', tfidf_transformer),\n",
    "                  ('clf', nb_classifier)])\n",
    "\n",
    "parameters2 = {**count_params, **tfidf_params, **nb_params}\n",
    "\n",
    "grid2 = GridSearchCV(pipe2, parameters2, cv=cross_validator, scoring=score_method, n_jobs=-1, verbose=3)\n",
    "\n",
    "grid2.fit(X,y)\n",
    "\n",
    "grid2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Pipeline([('vect', CountVectorizer(binary=False, ngram_range=(1,2), stop_words=stopwords.words('english'))),\n",
    "                  ('tfidf', TfidfTransformer(norm='l2', use_idf=True)),\n",
    "                  ('clf', MultinomialNB(alpha=0.1, fit_prior=False))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 (Naive Bayes, no TFIDF,  stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed: 16.6min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 24.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.1,\n",
       " 'clf__fit_prior': False,\n",
       " 'vect__binary': False,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__stop_words': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe3 = Pipeline([('vect', stem_count_vectorizer),\n",
    "                  ('clf', nb_classifier)])\n",
    "\n",
    "parameters3 = {**count_params, **nb_params}\n",
    "\n",
    "grid3 = GridSearchCV(pipe3, parameters3, cv=cross_validator, scoring=score_method, n_jobs=-1, verbose=3)\n",
    "\n",
    "grid3.fit(X,y)\n",
    "\n",
    "grid3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Pipeline([('vect', StemCountVectorizer(binary=False, ngram_range = (1,1), stop_words = None)),\n",
    "                  ('clf', MultinomialNB(alpha=0.1, fit_prior=False))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 (Naive Bayes, TFIDF, stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1128 tasks      | elapsed: 36.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1544 tasks      | elapsed: 50.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2024 tasks      | elapsed: 67.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2568 tasks      | elapsed: 85.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed: 95.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01,\n",
       " 'clf__fit_prior': False,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__binary': False,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__stop_words': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe4 = Pipeline([('vect', stem_count_vectorizer),\n",
    "                  ('tfidf', tfidf_transformer),\n",
    "                  ('clf', nb_classifier)])\n",
    "\n",
    "parameters4 = {**count_params, **tfidf_params, **nb_params}\n",
    "\n",
    "grid4 = GridSearchCV(pipe4, parameters4, cv=cross_validator, scoring=score_method, n_jobs=-1, verbose=3)\n",
    "\n",
    "grid4.fit(X,y)\n",
    "\n",
    "grid4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Pipeline([('vect', StemCountVectorizer(binary=False, ngram_range=(1,2), stop_words=None)),\n",
    "                  ('tfidf', TfidfTransformer(norm='l2', use_idf=True)),\n",
    "                  ('clf', MultinomialNB(alpha=0.01, fit_prior=False))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 (Random Forest, no TFIDF, no stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1512 candidates, totalling 7560 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   20.4s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1128 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1544 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2024 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2568 tasks      | elapsed: 22.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed: 24.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3848 tasks      | elapsed: 32.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4584 tasks      | elapsed: 37.9min\n",
      "[Parallel(n_jobs=-1)]: Done 5384 tasks      | elapsed: 40.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6248 tasks      | elapsed: 51.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 55.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7560 out of 7560 | elapsed: 56.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__class_weight': 'balanced',\n",
       " 'clf__criterion': 'gini',\n",
       " 'clf__max_depth': None,\n",
       " 'clf__max_features': 'sqrt',\n",
       " 'vect__binary': False,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__stop_words': ['i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  \"you're\",\n",
       "  \"you've\",\n",
       "  \"you'll\",\n",
       "  \"you'd\",\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  \"she's\",\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  \"it's\",\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  \"that'll\",\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  \"don't\",\n",
       "  'should',\n",
       "  \"should've\",\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  \"aren't\",\n",
       "  'couldn',\n",
       "  \"couldn't\",\n",
       "  'didn',\n",
       "  \"didn't\",\n",
       "  'doesn',\n",
       "  \"doesn't\",\n",
       "  'hadn',\n",
       "  \"hadn't\",\n",
       "  'hasn',\n",
       "  \"hasn't\",\n",
       "  'haven',\n",
       "  \"haven't\",\n",
       "  'isn',\n",
       "  \"isn't\",\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  \"mightn't\",\n",
       "  'mustn',\n",
       "  \"mustn't\",\n",
       "  'needn',\n",
       "  \"needn't\",\n",
       "  'shan',\n",
       "  \"shan't\",\n",
       "  'shouldn',\n",
       "  \"shouldn't\",\n",
       "  'wasn',\n",
       "  \"wasn't\",\n",
       "  'weren',\n",
       "  \"weren't\",\n",
       "  'won',\n",
       "  \"won't\",\n",
       "  'wouldn',\n",
       "  \"wouldn't\"]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe5 = Pipeline([('vect', count_vectorizer), \n",
    "                  ('clf', rf_classifier)])\n",
    "\n",
    "parameters5 = {**count_params, **rf_params}\n",
    "\n",
    "grid5 = GridSearchCV(pipe5, parameters5, cv=cross_validator, scoring=score_method, n_jobs=-1, verbose=3)\n",
    "\n",
    "grid5.fit(X,y)\n",
    "\n",
    "grid5.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Pipeline([('vect', CountVectorizer(binary=False, ngram_range = (1,2), stop_words = stopwords.words('english'))),\n",
    "                  ('clf', RandomForestClassifier(class_weight='balanced', criterion=\"gini\", max_depth=None, max_features=\"sqrt\",\n",
    "                                                 n_jobs=-1, random_state=RANDOM_SEED))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6 (Random Forest, TFIDF, no stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6048 candidates, totalling 30240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  8.0min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-398ddfa72b67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mgrid6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcross_validator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscore_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mgrid6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mgrid6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipe6 = Pipeline([('vect', count_vectorizer),\n",
    "                  ('tfidf', tfidf_transformer),\n",
    "                  ('clf', rf_classifier)])\n",
    "\n",
    "parameters6 = {**count_params, **tfidf_params, **rf_params}\n",
    "\n",
    "grid6 = GridSearchCV(pipe6, parameters6, cv=cross_validator, scoring=score_method, n_jobs=-1, verbose=3)\n",
    "\n",
    "grid6.fit(X,y)\n",
    "\n",
    "grid6.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = Pipeline([('vect', CountVectorizer(binary=True, ngram_range=(1,1), stop_words=stopwords.words('english'))),\n",
    "                  ('tfidf', TfidfTransformer(norm='l2', use_idf=True)),\n",
    "                  ('clf', RandomForestClassifier(class_weight='balanced', criterion=\"gini\", max_depth=None, max_features=\"log2\",\n",
    "                                                 n_jobs=-1, random_state=RANDOM_SEED))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7 (Random Forest, no TFIDF,  stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1512 candidates, totalling 7560 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   36.7s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed: 24.6min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 34.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1128 tasks      | elapsed: 47.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1544 tasks      | elapsed: 62.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2024 tasks      | elapsed: 85.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2568 tasks      | elapsed: 105.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed: 127.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3848 tasks      | elapsed: 155.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4584 tasks      | elapsed: 183.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5384 tasks      | elapsed: 211.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6248 tasks      | elapsed: 250.6min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 285.7min\n",
      "[Parallel(n_jobs=-1)]: Done 7560 out of 7560 | elapsed: 300.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__class_weight': 'balanced',\n",
       " 'clf__criterion': 'gini',\n",
       " 'clf__max_depth': 12,\n",
       " 'clf__max_features': 'log2',\n",
       " 'vect__binary': True,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__stop_words': ['i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  \"you're\",\n",
       "  \"you've\",\n",
       "  \"you'll\",\n",
       "  \"you'd\",\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  \"she's\",\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  \"it's\",\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  \"that'll\",\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  \"don't\",\n",
       "  'should',\n",
       "  \"should've\",\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  \"aren't\",\n",
       "  'couldn',\n",
       "  \"couldn't\",\n",
       "  'didn',\n",
       "  \"didn't\",\n",
       "  'doesn',\n",
       "  \"doesn't\",\n",
       "  'hadn',\n",
       "  \"hadn't\",\n",
       "  'hasn',\n",
       "  \"hasn't\",\n",
       "  'haven',\n",
       "  \"haven't\",\n",
       "  'isn',\n",
       "  \"isn't\",\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  \"mightn't\",\n",
       "  'mustn',\n",
       "  \"mustn't\",\n",
       "  'needn',\n",
       "  \"needn't\",\n",
       "  'shan',\n",
       "  \"shan't\",\n",
       "  'shouldn',\n",
       "  \"shouldn't\",\n",
       "  'wasn',\n",
       "  \"wasn't\",\n",
       "  'weren',\n",
       "  \"weren't\",\n",
       "  'won',\n",
       "  \"won't\",\n",
       "  'wouldn',\n",
       "  \"wouldn't\"]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe7 = Pipeline([('vect', stem_count_vectorizer),\n",
    "                  ('clf', rf_classifier)])\n",
    "\n",
    "parameters7 = {**count_params, **rf_params}\n",
    "\n",
    "grid7 = GridSearchCV(pipe7, parameters7, cv=cross_validator, scoring=score_method, n_jobs=-1, verbose=3)\n",
    "\n",
    "grid7.fit(X,y)\n",
    "\n",
    "grid7.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = Pipeline([('vect', CountVectorizer(binary=True, ngram_range = (1,1), stop_words = stopwords.words('english'))),\n",
    "                  ('clf', RandomForestClassifier(class_weight='balanced', criterion=\"gini\", max_depth=12, max_features=\"log2\",\n",
    "                                                 n_jobs=-1, random_state=RANDOM_SEED))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8 (Random Forest, TFIDF, stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe8 = Pipeline([('vect', stem_count_vectorizer),\n",
    "                  ('tfidf', tfidf_transformer),\n",
    "                  ('clf', rf_classifier)])\n",
    "\n",
    "parameters8 = {**count_params, **tfidf_params, **rf_params}\n",
    "\n",
    "grid8 = GridSearchCV(pipe8, parameters8, cv=cross_validator, scoring=score_method, n_jobs=-1, verbose=3)\n",
    "\n",
    "grid8.fit(X,y)\n",
    "\n",
    "grid8.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 9 (SVM, no TFIDF, no stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe9 = Pipeline([('vect', count_vectorizer), \n",
    "                  ('clf', svm_classifier)])\n",
    "\n",
    "parameters9 = {**count_params, **svm_params}\n",
    "\n",
    "grid9 = GridSearchCV(pipe9, parameters9, cv=cross_validator, scoring=score_method, n_jobs=-1, verbose=3)\n",
    "\n",
    "grid9.fit(X,y)\n",
    "\n",
    "grid9.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = Pipeline([('vect', CountVectorizer(binary=True, ngram_range = (1,1), stop_words = stopwords.words('english'))),\n",
    "                  ('clf', SVC(C=0.1, class_weight='balanced', degree=2, gamma=\"scale\", kernel='linear'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 10 (SVM, TFIDF, no stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe10 = Pipeline([('vect', count_vectorizer),\n",
    "                  ('tfidf', tfidf_transformer),\n",
    "                  ('clf', svm_classifier)])\n",
    "\n",
    "parameters10 = {**count_params, **tfidf_params, **svm_params}\n",
    "\n",
    "grid10 = GridSearchCV(pipe10, parameters10, cv=cross_validator, scoring=score_method, n_jobs=-1, verbose=3)\n",
    "\n",
    "grid10.fit(X,y)\n",
    "\n",
    "grid10.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = Pipeline([('vect', CountVectorizer(binary=True, ngram_range = (1,2), stop_words = stopwords.words('english'))),\n",
    "                    ('tfidf', TfidfTransformer(norm='l1', use_idf=True)),\n",
    "                  ('clf', SVC(C=100, class_weight=None, degree=2, gamma=\"scale\", kernel='linear'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 11 (SVM, no TFIDF,  stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6480 candidates, totalling 32400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 26.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1128 tasks      | elapsed: 38.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1544 tasks      | elapsed: 52.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2024 tasks      | elapsed: 69.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2568 tasks      | elapsed: 88.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed: 109.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3848 tasks      | elapsed: 131.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4584 tasks      | elapsed: 157.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5384 tasks      | elapsed: 184.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6248 tasks      | elapsed: 214.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 246.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8168 tasks      | elapsed: 281.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9224 tasks      | elapsed: 318.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10344 tasks      | elapsed: 357.9min\n",
      "[Parallel(n_jobs=-1)]: Done 11528 tasks      | elapsed: 392.9min\n",
      "[Parallel(n_jobs=-1)]: Done 12776 tasks      | elapsed: 426.0min\n",
      "[Parallel(n_jobs=-1)]: Done 14088 tasks      | elapsed: 460.7min\n",
      "[Parallel(n_jobs=-1)]: Done 15464 tasks      | elapsed: 497.0min\n",
      "[Parallel(n_jobs=-1)]: Done 16904 tasks      | elapsed: 539.8min\n",
      "[Parallel(n_jobs=-1)]: Done 18408 tasks      | elapsed: 592.3min\n",
      "[Parallel(n_jobs=-1)]: Done 19976 tasks      | elapsed: 647.3min\n",
      "[Parallel(n_jobs=-1)]: Done 21608 tasks      | elapsed: 704.1min\n",
      "[Parallel(n_jobs=-1)]: Done 23304 tasks      | elapsed: 762.4min\n",
      "[Parallel(n_jobs=-1)]: Done 25064 tasks      | elapsed: 822.6min\n",
      "[Parallel(n_jobs=-1)]: Done 26888 tasks      | elapsed: 887.4min\n",
      "[Parallel(n_jobs=-1)]: Done 28776 tasks      | elapsed: 953.5min\n",
      "[Parallel(n_jobs=-1)]: Done 30728 tasks      | elapsed: 1020.2min\n",
      "[Parallel(n_jobs=-1)]: Done 32400 out of 32400 | elapsed: 1077.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__C': 0.1,\n",
       " 'clf__class_weight': 'balanced',\n",
       " 'clf__degree': 2,\n",
       " 'clf__gamma': 'scale',\n",
       " 'clf__kernel': 'linear',\n",
       " 'vect__binary': True,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__stop_words': ['i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  \"you're\",\n",
       "  \"you've\",\n",
       "  \"you'll\",\n",
       "  \"you'd\",\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  \"she's\",\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  \"it's\",\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  \"that'll\",\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  \"don't\",\n",
       "  'should',\n",
       "  \"should've\",\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  \"aren't\",\n",
       "  'couldn',\n",
       "  \"couldn't\",\n",
       "  'didn',\n",
       "  \"didn't\",\n",
       "  'doesn',\n",
       "  \"doesn't\",\n",
       "  'hadn',\n",
       "  \"hadn't\",\n",
       "  'hasn',\n",
       "  \"hasn't\",\n",
       "  'haven',\n",
       "  \"haven't\",\n",
       "  'isn',\n",
       "  \"isn't\",\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  \"mightn't\",\n",
       "  'mustn',\n",
       "  \"mustn't\",\n",
       "  'needn',\n",
       "  \"needn't\",\n",
       "  'shan',\n",
       "  \"shan't\",\n",
       "  'shouldn',\n",
       "  \"shouldn't\",\n",
       "  'wasn',\n",
       "  \"wasn't\",\n",
       "  'weren',\n",
       "  \"weren't\",\n",
       "  'won',\n",
       "  \"won't\",\n",
       "  'wouldn',\n",
       "  \"wouldn't\"]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe11 = Pipeline([('vect', stem_count_vectorizer),\n",
    "                  ('clf', svm_classifier)])\n",
    "\n",
    "parameters11 = {**count_params, **svm_params}\n",
    "\n",
    "grid11 = GridSearchCV(pipe11, parameters11, cv=cross_validator, scoring=score_method, n_jobs=-1, verbose=3)\n",
    "\n",
    "grid11.fit(X,y)\n",
    "\n",
    "grid11.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 12 (SVM, TFIDF, stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder:** Model 12 was not trained using a GridSearch because the estimated training time was too large (90 hours).  Instead, we take the parameters from model 10 (which is the same architecture minus stemming) and apply stemming.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model12 = Pipeline([('vect', StemCountVectorizer(binary=True, ngram_range = (1,2), stop_words = stopwords.words('english'))),\n",
    "                    ('tfidf', TfidfTransformer(norm='l1', use_idf=True)),\n",
    "                  ('clf', SVC(C=100, class_weight=None, degree=2, gamma=\"scale\", kernel='linear'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "Compairons of TFIDF vs no TFIDF\n",
    "\n",
    "Comparisons of Stemming vs no stemming\n",
    "\n",
    "Comparisons of classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Scores\n",
    "\n",
    "To compare the models generated, the cell below performs cross validation using all of the data and reports the balanced accuracy of each model.  Balanced accuracy was used because the number of examples in each class is unequal.  \n",
    "\n",
    "The use of all of the data to estimate the balanced accuracies of the models poses a problem.  Because the hyperparameters were tunned using `GridSearchCV` on all of our data, the models are being evaluated on the same data that was used to construct them.  This could inflate their balanced accuracies higher than what might be observed on new, unseen data.  One solution to this problem, nested cross validation, was tried, but not used in the final models because it would be too computationally expensive to compute.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12]\n",
    "\n",
    "cv_score_table = pd.DataFrame()\n",
    "\n",
    "for i in range(0,len(models)):\n",
    "    results = cross_val_score(models[i], X, y, cv=cross_validator, scoring=score_method, n_jobs=-1)\n",
    "    cv_score_table.insert(i, \"model \" + str(i+1), results, True)\n",
    "\n",
    "cv_score_table.index.name = \"fold\"\n",
    "cv_score_table.loc[\"mean\"] = cv_score_table.mean()\n",
    "cv_score_table.loc[\"std\"] = cv_score_table.std()\n",
    "cv_score_table.loc[\"max\"] = cv_score_table.max()\n",
    "\n",
    "cv_score_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Below is a confusion matrix generated by training our best model, model **BLANK** on a subset of the data.  This yields a more accurate balanced accuracy than reported above because the model here is being tested using data it was not trained with.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    ".fit(X_train,y_train)\n",
    "yhat = .predict(X_test)\n",
    "\n",
    "print(\"balanced_accuracy:\", metrics.balanced_accuracy_score(y_test, yhat))\n",
    "\n",
    "plot = metrics.plot_confusion_matrix(, X_test, y_test,\n",
    "                             values_format = 'd',\n",
    "                             cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, axes=None, cv=None,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes, verbose=3)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "title = \n",
    "\n",
    "plot_learning_curve(, title, X, y, cv=cross_validator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "## Project Improvements\n",
    "\n",
    "## Takeaways\n",
    "\n",
    "In this project I learned a lot about NLP techniques, as well as learned about models I hadn't been exposed to before. New to me were the concepts of turning a document into a word count vector, stemming, TF / IDF transformations, and stop words.  As far as models go, this was also the first time that I got to create a Naive Bayes, SVM, and Random Forest model.  Using Scikit-learn in this context also helped establish a set of skills which I can apply to other projects. \n",
    "\n",
    "## Moving Forward\n",
    "\n",
    "Moving forward, I would like to continue to explore the models and techniques used in this project.  I believe that what hindered my progress through this project was a lack of knowledge about which parameters are most significant to a model's success.  Although I read and learned about what each parameter does, I was unsure about which would have the biggest change on a model.  This is why I performed such vast grid searches.  I almost wonder if my time would have been better exploring more models and processing techniques, such as Word2Vec or neural networks, than waiting for these models to train.  \n",
    "\n",
    "I would also like to be more scientific and organized when I start my next project.  In this project, I began by just trying different things, whereas I would have liked to started with more of a plan.  I also would like to think about how and on what machine I train, so I can not waste time.  In future projects, I think having the experience I gained here will be a big help when constructing a more robust methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "Ben-Hur, Asa. “CS345: Machine Learning Foundations and Practice.” GitHub, Colorado State University, 7 Dec. 2020, www.github.com/asabenhur/CS345. \n",
    "\n",
    "Shaikh, Javed. “Machine Learning, NLP: Text Classification Using Scikit-Learn, Python and NLTK.” Medium, Towards Data Science, 30 Oct. 2017, www.towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a. \n",
    "\n",
    "Starmer, Josh. “Naive Bayes, Clearly Explained!!!” YouTube, StatQuest with Josh Starmer, 3 June 2020, www.youtube.com/watch?v=O2L2Uv9pdDA. \n",
    "\n",
    "Starmer, Josh. “Support Vector Machines, Clearly Explained!!!” YouTube, StatQuest with Josh Starmer, 30 September 2019, www.youtube.com/watch?v=O2L2Uv9pdDA. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
